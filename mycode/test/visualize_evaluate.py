import argparse
import os
import torch
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import json
from tqdm import tqdm
import cv2
from collections import defaultdict
import torchvision.ops.boxes as box_ops

# å¼•å…¥ GroundingDINO çš„å¿…è¦æ¨¡å—
import groundingdino.datasets.transforms as T
from groundingdino.models import build_model
from groundingdino.util.slconfig import SLConfig
from groundingdino.util.utils import clean_state_dict
# å¼•å…¥å®˜æ–¹æ¨ç†å·¥å…·
from groundingdino.util.inference import predict, annotate

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹é…ç½®
CONFIG_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/config/cfg_odvg.py"
CHECKPOINT_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/logs/railway_4gpu_wandb_full_label/checkpoint_best_regular.pth"
TEST_JSON_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/test_split_coco_fixed.json"
IMAGE_ROOT = "/opt/data/private/xjx/RailMind/database/test/åŸºå‡†æµ‹è¯•_1229/åŸºå‡†æµ‹è¯•æ•°æ®é›†"
LABEL_MAP_FILE = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/label_map.json"
OUTPUT_DIR = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/mycode/0110_full_test_benchmark" # å»ºè®®ä¿®æ”¹è¾“å‡ºç›®å½•åä»¥åŒºåˆ†
BERT_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/GroundingDINO/weights/bert-base-uncased"



# 2. é˜ˆå€¼è®¾ç½®
BOX_THRESHOLD = 0.25
TEXT_THRESHOLD = 0.25
IOU_THRESHOLD = 0.5

# 3. [æ–°å¢] Prompt æ¨¡å¼é€‰æ‹©
# True: ä»…ä½¿ç”¨è¯¥å›¾ç‰‡çœŸå®åŒ…å«çš„æ ‡ç­¾ (Oracle Mode)
# False: ä½¿ç”¨ Label Map ä¸­æ‰€æœ‰æ ‡ç­¾ (Standard Mode)
USE_GT_LABELS_ONLY = False
# =================================================

# ... [SceneEvaluator ç±»ä¿æŒä¸å˜] ...
class SceneEvaluator:
    def __init__(self, iou_threshold=0.5):
        self.iou_threshold = iou_threshold
        # æ•°æ®ç»“æ„: {scene_name: {class_name: {'tp': [], 'fp': [], 'scores': [], 'num_gt': 0}}}
        self.stats = defaultdict(lambda: defaultdict(lambda: {'tp': [], 'fp': [], 'scores': [], 'num_gt': 0}))
        
    def update(self, scene, pred_boxes, pred_scores, pred_labels, gt_boxes, gt_labels):
        unique_labels = set(pred_labels) | set(gt_labels)
        
        for label in unique_labels:
            p_indices = [i for i, x in enumerate(pred_labels) if x == label]
            g_indices = [i for i, x in enumerate(gt_labels) if x == label]
            
            p_boxes_cls = pred_boxes[p_indices] if len(p_indices) > 0 else torch.empty((0, 4))
            p_scores_cls = pred_scores[p_indices] if len(p_indices) > 0 else torch.empty((0,))
            g_boxes_cls = gt_boxes[g_indices] if len(g_indices) > 0 else torch.empty((0, 4))
            
            self.stats[scene][label]['num_gt'] += len(g_boxes_cls)
            
            if len(p_boxes_cls) == 0:
                continue

            if len(g_boxes_cls) == 0:
                self.stats[scene][label]['fp'].extend([1] * len(p_boxes_cls))
                self.stats[scene][label]['tp'].extend([0] * len(p_boxes_cls))
                self.stats[scene][label]['scores'].extend(p_scores_cls.tolist())
                continue

            ious = box_ops.box_iou(p_boxes_cls, g_boxes_cls) 
            gt_detected = torch.zeros(len(g_boxes_cls), dtype=torch.bool)
            sorted_indices = torch.argsort(p_scores_cls, descending=True)
            
            for idx in sorted_indices:
                max_iou, max_gt_idx = torch.max(ious[idx], dim=0)
                is_tp = False
                if max_iou >= self.iou_threshold:
                    if not gt_detected[max_gt_idx]:
                        gt_detected[max_gt_idx] = True
                        is_tp = True
                
                self.stats[scene][label]['tp'].append(1 if is_tp else 0)
                self.stats[scene][label]['fp'].append(0 if is_tp else 1)
                self.stats[scene][label]['scores'].append(p_scores_cls[idx].item())

    def calculate_ap(self, tp, fp, n_pos):
        if n_pos == 0: return 0.0
        tp = np.cumsum(tp)
        fp = np.cumsum(fp)
        rec = tp / n_pos
        prec = tp / (tp + fp + 1e-6)
        ap = 0.0
        for t in np.arange(0.0, 1.1, 0.1):
            if np.sum(rec >= t) == 0: p = 0
            else: p = np.max(prec[rec >= t])
            ap += p / 11.0
        return ap

    def print_results(self):
        print("\n" + "="*100)
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ (IoU Threshold = {self.iou_threshold}) | Mode: {'GT Labels Only' if USE_GT_LABELS_ONLY else 'All Labels'}")
        print("="*100)
        header = "{:<30} | {:<15} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8}".format(
            "Scene / Class", "Precision", "Recall", "AP@50", "GT Count", "Pred Count", "TP", "FP")
        print(header)
        print("-" * 120)

        total_tp, total_fp, total_gt = 0, 0, 0
        for scene, class_data in self.stats.items():
            print(f"ğŸ“‚ åœºæ™¯: {scene}")
            scene_tp, scene_fp, scene_gt = 0, 0, 0
            for label, data in class_data.items():
                tp, fp = np.array(data['tp']), np.array(data['fp'])
                num_gt = data['num_gt']
                sum_tp, sum_fp = (np.sum(tp) if len(tp)>0 else 0), (np.sum(fp) if len(fp)>0 else 0)
                scene_tp += sum_tp; scene_fp += sum_fp; scene_gt += num_gt
                
                precision = sum_tp / (sum_tp + sum_fp + 1e-6)
                recall = sum_tp / (num_gt + 1e-6)
                ap = self.calculate_ap(tp, fp, num_gt)
                
                print("{:<30} | {:.4f}          | {:.4f}   | {:.4f}   | {:<8} | {:<8} | {:<8} | {:<8}".format(
                    f"  â”œâ”€ {label}", precision, recall, ap, num_gt, len(tp), int(sum_tp), int(sum_fp)))
            
            s_prec = scene_tp / (scene_tp + scene_fp + 1e-6)
            s_rec = scene_tp / (scene_gt + 1e-6)
            print("{:<30} | {:.4f}          | {:.4f}   | -        | {:<8} | {:<8} | {:<8} | {:<8}".format(
                f"  â””â”€ [Scene Total]", s_prec, s_rec, scene_gt, int(scene_tp + scene_fp), int(scene_tp), int(scene_fp)))
            print("-" * 120)
            total_tp += scene_tp; total_fp += scene_fp; total_gt += scene_gt

        all_prec = total_tp / (total_tp + total_fp + 1e-6)
        all_rec = total_tp / (total_gt + 1e-6)
        print("="*100)
        print(f"ğŸ† æ€»ä½“æ¦‚è§ˆ (Overall): Precision: {all_prec:.4f} | Recall: {all_rec:.4f} | GT: {total_gt} | TP: {int(total_tp)} | FP: {int(total_fp)} | FN: {int(total_gt - total_tp)}")
        print("="*100)

# ... [å·¥å…·å‡½æ•° load_model, load_image, coco_box_to_xyxy ä¿æŒä¸å˜] ...
def load_model(model_config_path, model_checkpoint_path, device="cuda"):
    args = SLConfig.fromfile(model_config_path)
    print(f"ğŸ”„ å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° BERT è·¯å¾„: {BERT_PATH}")
    args.text_encoder_type = BERT_PATH
    args.device = device
    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    model.eval()
    return model.to(device)

def load_image(image_path):
    image_pil = Image.open(image_path).convert("RGB")
    transform = T.Compose([
        T.RandomResize([800], max_size=1333),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    image, _ = transform(image_pil, None)
    return image_pil, image

def coco_box_to_xyxy(box):
    x, y, w, h = box
    return [x, y, x + w, y + h]

def main():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # 1. å‡†å¤‡å…¨å±€ Label Map 
    print(f"ğŸ“– è¯»å– Label Map: {LABEL_MAP_FILE}")
    with open(LABEL_MAP_FILE, 'r') as f:
        label_map = json.load(f)
    
    id_to_name = {int(k): v for k, v in label_map.items()}
    all_class_names = list(label_map.values())
    all_class_names = [str(name) for name in all_class_names if isinstance(name, (str, int))]
    
    # æ„é€  å…¨é‡ Prompt
    FULL_PROMPT = " . ".join(all_class_names) + " ."
    
    if USE_GT_LABELS_ONLY:
        print(f"âš ï¸ æ¨¡å¼: [GT Labels Only] - ä»…ä½¿ç”¨å›¾ç‰‡ä¸­çœŸå®å­˜åœ¨çš„æ ‡ç­¾è¿›è¡Œæ£€æµ‹")
    else:
        print(f"âš ï¸ æ¨¡å¼: [All Labels] - ä½¿ç”¨æ‰€æœ‰æ ‡ç­¾è¿›è¡Œæ£€æµ‹")
        print(f"ğŸ“ å…¨é‡ Prompt: {FULL_PROMPT[:50]}...")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = load_model(CONFIG_PATH, CHECKPOINT_PATH, device)

    # 2. è¯»å–æ•°æ®
    with open(TEST_JSON_PATH, 'r') as f:
        coco_data = json.load(f)
    
    gt_dict = defaultdict(list)
    for ann in coco_data['annotations']:
        gt_dict[ann['image_id']].append(ann)
    
    images_info = coco_data['images']
    print(f"ğŸ“Š å‡†å¤‡å¤„ç† {len(images_info)} å¼ å›¾ç‰‡...")

    evaluator = SceneEvaluator(iou_threshold=IOU_THRESHOLD)
    
    # æ˜¾å¼æ¸…ç©ºæ˜¾å­˜
    torch.cuda.empty_cache()

    # 3. æ¨ç†å¾ªç¯
    with torch.no_grad():
        for img_info in tqdm(images_info):
            file_name = img_info['file_name']
            img_id = img_info['id']
            full_image_path = os.path.join(IMAGE_ROOT, file_name)
            scene_name = os.path.dirname(file_name) 
            if scene_name == "": scene_name = "Root"

            if not os.path.exists(full_image_path):
                continue

            # =========================================================
            # [æ ¸å¿ƒä¿®æ”¹] åŠ¨æ€æ„é€  Prompt
            # =========================================================
            
            # 1. è·å–è¯¥å›¾æ‰€æœ‰çš„ GT Category ID
            current_gt_anns = gt_dict.get(img_id, [])
            
            # 2. ç¡®å®šå½“å‰ä½¿ç”¨çš„ Prompt
            current_prompt = ""
            
            if USE_GT_LABELS_ONLY:
                if len(current_gt_anns) == 0:
                    # å¦‚æœè¯¥å›¾æ²¡æœ‰ä»»ä½•æ ‡æ³¨ (è´Ÿæ ·æœ¬)ï¼Œæˆ‘ä»¬å¦‚ä½•å¤„ç†ï¼Ÿ
                    # ç­–ç•¥A: è·³è¿‡æ£€æµ‹ (å› ä¸ºæ²¡æœ‰ç›®æ ‡å¯ä»¥æ£€æµ‹) -> è¿™æ ·ç®—å‡ºæ¥å…¨æ˜¯ TN
                    # ç­–ç•¥B: ä»ç„¶ç»™å…¨é‡æ ‡ç­¾æµ‹è¯•è¯¯æ£€ -> è¿™æ ·æ›´æœ‰æ„ä¹‰
                    # è¿™é‡Œé‡‡ç”¨ç­–ç•¥Bï¼Œæˆ–è€…ä½ å¯ä»¥ç»™ä¸€ä¸ªç©º promptï¼Œä½† GroundingDINO å¯èƒ½æŠ¥é”™
                    # ä¸ºäº†é¿å…æŠ¥é”™ï¼Œå¦‚æœæ˜¯ç©ºæ ‡æ³¨å›¾ç‰‡ï¼Œæˆ‘ä»¬ç»™ä¸€ä¸ª "random" æˆ–è€…æ²¿ç”¨ Full Prompt
                    # å»ºè®®ï¼šå¦‚æœæ˜¯ç©ºæ ‡æ³¨ï¼Œè·³è¿‡æœ¬æ¬¡å¾ªç¯çš„ inferenceï¼Œæˆ–è€…ç»™ä¸€ä¸ªå¿…å®šä¸å­˜åœ¨çš„ç±»
                    if len(current_gt_anns) == 0:
                        # è¿™æ˜¯ä¸€ä¸ªåªæœ‰èƒŒæ™¯çš„å›¾ï¼Œä¸ºäº†æµ‹è¯•è¯¯æ£€ï¼Œæˆ‘ä»¬å¯ä»¥éšä¾¿ç»™ä¸€ä¸ª Prompt
                        # æˆ–è€…è·³è¿‡ã€‚è¿™é‡Œé€‰æ‹©è·³è¿‡ Inferenceï¼Œç›´æ¥è®°å½• GT=0
                        # ä¹Ÿå¯ä»¥é€‰æ‹©ç»™ä¸€ä¸ª 'object .' çœ‹å®ƒä¼šä¸ä¼šä¹±æ£€
                        current_prompt = "object ." 
                    else:
                        # æå–è¯¥å›¾åŒ…å«çš„å”¯ä¸€ç±»åˆ«åç§°
                        unique_cat_ids = set([ann['category_id'] for ann in current_gt_anns])
                        unique_names = [id_to_name.get(cid, str(cid)) for cid in unique_cat_ids]
                        current_prompt = " . ".join(unique_names) + " ."
                else:
                    # æœ‰æ ‡æ³¨ï¼Œä½¿ç”¨ GT ç±»åˆ«
                    unique_cat_ids = set([ann['category_id'] for ann in current_gt_anns])
                    unique_names = [id_to_name.get(cid, str(cid)) for cid in unique_cat_ids]
                    current_prompt = " . ".join(unique_names) + " ."
            else:
                # ä½¿ç”¨å…¨é‡æ ‡ç­¾
                current_prompt = FULL_PROMPT

            # 3. åŠ è½½å›¾ç‰‡ä¸æ¨ç†
            image_source, image = load_image(full_image_path)
            img_w, img_h = image_source.size

            boxes, logits, phrases = predict(
                model=model,
                image=image,
                caption=current_prompt, # ä½¿ç”¨åŠ¨æ€æ„å»ºçš„ prompt
                box_threshold=BOX_THRESHOLD,
                text_threshold=TEXT_THRESHOLD,
                device=device
            )

            # =========================================================
            # æ•°æ®å‡†å¤‡ (ä¸ä¹‹å‰ç›¸åŒ)
            # =========================================================
            pred_boxes_xyxy = []
            pred_scores = []
            pred_labels = []
            json_results = []

            for box, score, label in zip(boxes, logits, phrases):
                box_norm = box.tolist() 
                cx, cy, w, h = box_norm
                x1 = (cx - w/2) * img_w
                y1 = (cy - h/2) * img_h
                x2 = (cx + w/2) * img_w
                y2 = (cy + h/2) * img_h
                
                pred_boxes_xyxy.append([x1, y1, x2, y2])
                pred_scores.append(score.item())
                pred_labels.append(label)

                json_results.append({
                    "label": label,
                    "score": round(score.item(), 4),
                    "box_pixel_xyxy": [int(x1), int(y1), int(x2), int(y2)]
                })

            # å¤„ç† GT
            gt_boxes_xyxy = []
            gt_labels = []
            for ann in current_gt_anns:
                xyxy = coco_box_to_xyxy(ann['bbox'])
                gt_boxes_xyxy.append(xyxy)
                cat_name = id_to_name.get(ann['category_id'], str(ann['category_id'])) 
                gt_labels.append(cat_name)

            # æ›´æ–°è¯„ä¼°å™¨
            if len(pred_boxes_xyxy) > 0:
                t_pred_boxes = torch.tensor(pred_boxes_xyxy)
                t_pred_scores = torch.tensor(pred_scores)
            else:
                t_pred_boxes = torch.empty((0, 4))
                t_pred_scores = torch.empty((0,))
                
            if len(gt_boxes_xyxy) > 0:
                t_gt_boxes = torch.tensor(gt_boxes_xyxy)
            else:
                t_gt_boxes = torch.empty((0, 4))
                
            evaluator.update(scene_name, t_pred_boxes, t_pred_scores, pred_labels, t_gt_boxes, gt_labels)

            # ä¿å­˜ç»“æœ
            base_name = os.path.basename(file_name)
            json_save_name = "vis_" + os.path.splitext(base_name)[0] + ".json"
            json_save_path = os.path.join(OUTPUT_DIR, json_save_name)
            with open(json_save_path, "w", encoding='utf-8') as f_json:
                json.dump({"file_name": file_name, "height": img_h, "width": img_w, "objects": json_results}, f_json, indent=4, ensure_ascii=False)

            annotated_frame = annotate(image_source=np.asarray(image_source), boxes=boxes, logits=logits, phrases=phrases)
            cv2.imwrite(os.path.join(OUTPUT_DIR, "vis_" + os.path.basename(file_name)), annotated_frame)

    evaluator.print_results()
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()