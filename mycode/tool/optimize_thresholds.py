import os
import torch
import json
import numpy as np
from PIL import Image
from tqdm import tqdm
import groundingdino.datasets.transforms as T
from groundingdino.models import build_model
from groundingdino.util.slconfig import SLConfig
from groundingdino.util.utils import clean_state_dict
from groundingdino.util.inference import predict

# ================= âš™ï¸ åŸºç¡€é…ç½® =================
CONFIG_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/config/cfg_odvg.py"
CHECKPOINT_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/logs/railway_4gpu_80_10_10/checkpoint_best_regular.pth"
TEST_JSON_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/test_split_coco.json"
IMAGE_ROOT = "/opt/data/private/xjx/RailMind/é«˜é€Ÿé“è·¯æ— äººæœºå›¾åƒ/FilteredLabeled"
LABEL_MAP_FILE = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/label_map.json"
BERT_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/GroundingDINO/weights/bert-base-uncased"

# GT JSON æ‰€åœ¨çš„æ–‡ä»¶å¤¹ (ç”¨äºåŠ è½½ Ground Truth)
DIR_GT = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/mycode/vis_gt_results"

# åŒ¹é… IoU é˜ˆå€¼
IOU_THRESHOLD = 0.8
# ===============================================

def load_model(model_config_path, model_checkpoint_path, device="cuda"):
    args = SLConfig.fromfile(model_config_path)
    args.text_encoder_type = BERT_PATH
    args.device = device
    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
    model.eval()
    return model.to(device)

def load_image(image_path):
    image_pil = Image.open(image_path).convert("RGB")
    transform = T.Compose([
        T.RandomResize([800], max_size=1333),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    image, _ = transform(image_pil, None)
    return image_pil, image

def compute_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ª [x1, y1, x2, y2] çŸ©å½¢çš„ IoU"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    return intersection / union if union > 0 else 0

def load_gt_detections(json_path):
    """åŠ è½½çœŸå®æ ‡æ³¨ GT"""
    if not os.path.exists(json_path):
        return []
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    normalized_objs = []
    if 'objects' not in data: return []
    for obj in data['objects']:
        if 'box_pixel_xyxy' in obj:
            box = obj['box_pixel_xyxy']
        elif 'bbox' in obj:
             continue
        else:
            continue
        label = obj.get('label', 'unknown')
        normalized_objs.append({'label': label, 'box': box})
    return normalized_objs

def compare_single_pair(objs_gt, objs_pred):
    """å†…å­˜ä¸­å¯¹æ¯”ä¸€å¯¹ç»“æœ"""
    stats = {'matched': 0, 'missed': 0, 'extra': 0}
    matched_pred_indices = set()
    
    for gt in objs_gt:
        best_iou = -1
        best_idx = -1
        for idx, pred in enumerate(objs_pred):
            if idx in matched_pred_indices: continue
            if gt['label'] != pred['label']: continue
            iou = compute_iou(gt['box'], pred['box'])
            if iou > best_iou:
                best_iou = iou
                best_idx = idx
        
        if best_iou >= IOU_THRESHOLD:
            matched_pred_indices.add(best_idx)
            stats['matched'] += 1
        else:
            stats['missed'] += 1
            
    stats['extra'] = len(objs_pred) - len(matched_pred_indices)
    return stats

def run_evaluation(model, device, text_prompt, images_info, threshold, gt_map):
    """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„è¯„ä¼°å¾ªç¯"""
    total_stats = {'gt': 0, 'pred': 0, 'matched': 0, 'missed': 0, 'extra': 0}
    
    # éå†æ‰€æœ‰å›¾ç‰‡
    # ä¸ºäº†é€Ÿåº¦ï¼Œè¿™é‡Œä¸å†ç”¨ tqdm æ˜¾ç¤ºè¯¦ç»†è¿›åº¦æ¡ï¼Œåªåœ¨å¤–éƒ¨æ˜¾ç¤ºè½®æ¬¡
    for img_info in images_info:
        file_name = img_info['file_name']
        full_image_path = os.path.join(IMAGE_ROOT, file_name)
        
        # 1. æ‰¾åˆ°å¯¹åº”çš„ GT æ–‡ä»¶
        # å‡è®¾æ–‡ä»¶å gt_xxx.json å¯¹åº” xxx.jpg
        base_name = os.path.splitext(os.path.basename(file_name))[0]
        gt_filename = f"gt_{base_name}.json"
        
        # å¦‚æœæ²¡æœ‰ GT æ–‡ä»¶ï¼Œè·³è¿‡å¯¹æ¯”
        if base_name not in gt_map:
            continue
            
        gt_path = os.path.join(DIR_GT, gt_filename)
        objs_gt = load_gt_detections(gt_path)
        
        if not os.path.exists(full_image_path): continue

        # 2. æ¨¡å‹æ¨ç†
        image_pil, image = load_image(full_image_path)
        image = image.to(device)
        img_w, img_h = image_pil.size

        with torch.no_grad():
            boxes, logits, phrases = predict(
                model=model,
                image=image,
                caption=text_prompt,
                box_threshold=threshold,
                text_threshold=threshold,
                device=device
            )

        # 3. æ ¼å¼åŒ–é¢„æµ‹ç»“æœ
        objs_pred = []
        for box, score, label in zip(boxes, logits, phrases):
            box_norm = box.tolist()
            cx, cy, w, h = box_norm
            x1 = int((cx - w/2) * img_w)
            y1 = int((cy - h/2) * img_h)
            x2 = int((cx + w/2) * img_w)
            y2 = int((cy + h/2) * img_h)
            objs_pred.append({
                'label': label,
                'score': score.item(),
                'box': [x1, y1, x2, y2]
            })

        # 4. å¯¹æ¯”
        res = compare_single_pair(objs_gt, objs_pred)
        
        # 5. ç´¯åŠ 
        total_stats['gt'] += len(objs_gt)
        total_stats['pred'] += len(objs_pred)
        total_stats['matched'] += res['matched']
        total_stats['missed'] += res['missed']
        total_stats['extra'] += res['extra']

    # è®¡ç®—æŒ‡æ ‡
    recall = total_stats['matched'] / total_stats['gt'] if total_stats['gt'] > 0 else 0
    precision = total_stats['matched'] / total_stats['pred'] if total_stats['pred'] > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return recall, precision, f1, total_stats

def main():
    # 1. åˆå§‹åŒ–
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("ğŸš€ åŠ è½½æ¨¡å‹ä¸­...")
    model = load_model(CONFIG_PATH, CHECKPOINT_PATH, device)
    
    # 2. å‡†å¤‡ Prompt
    with open(LABEL_MAP_FILE, 'r') as f:
        label_map = json.load(f)
    class_names = [str(name) for name in label_map.values() if isinstance(name, (str, int))]
    text_prompt = " . ".join(class_names) + " ."
    
    # 3. è¯»å–æµ‹è¯•é›†åˆ—è¡¨
    with open(TEST_JSON_PATH, 'r') as f:
        coco_data = json.load(f)
    images_info = coco_data['images']
    
    # 4. é¢„å…ˆå»ºç«‹ GT ç´¢å¼•
    gt_files = [f for f in os.listdir(DIR_GT) if f.startswith("gt_")]
    gt_map = {f[3:].replace(".json", ""): f for f in gt_files} # key: "123", value: "gt_123.json"
    
    print(f"ğŸ“Š æµ‹è¯•é›†åŒ…å« {len(images_info)} å¼ å›¾ç‰‡ï¼Œæ‰¾åˆ° {len(gt_map)} ä¸ª GT æ–‡ä»¶ã€‚")
    print("ğŸ”„ å¼€å§‹é˜ˆå€¼æœç´¢ (0.10 -> 0.50)...")
    print("-" * 80)
    print(f"{'Threshold':<10} | {'Recall':<10} | {'Precision':<10} | {'F1-Score':<10} | {'TP':<6} | {'FP':<6} | {'FN':<6}")
    print("-" * 80)

    best_f1 = -1
    best_res = None
    best_thresh = -1
    
    # 5. å¾ªç¯é˜ˆå€¼
    # np.arange(0.1, 0.51, 0.05) ä¼šç”Ÿæˆ [0.1, 0.15, ..., 0.5]
    thresholds = np.arange(0.1, 0.51, 0.05)
    
    for thr in thresholds:
        thr = round(thr, 2) # é¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
        
        rec, prec, f1, stats = run_evaluation(model, device, text_prompt, images_info, thr, gt_map)
        
        print(f"{thr:<10} | {rec:<10.2%} | {prec:<10.2%} | {f1:<10.4f} | {stats['matched']:<6} | {stats['extra']:<6} | {stats['missed']:<6}")
        
        if f1 > best_f1:
            best_f1 = f1
            best_res = (rec, prec, f1, stats)
            best_thresh = thr

    print("-" * 80)
    print("\nğŸ† æœ€ä½³ç»“æœ (Best Result):")
    print(f"ğŸ”¥ æœ€ä½³é˜ˆå€¼ (Threshold): {best_thresh}")
    print(f"ğŸ”µ å¬å›ç‡ (Recall):    {best_res[0]:.2%}")
    print(f"ğŸ”´ ç²¾ç¡®ç‡ (Precision): {best_res[1]:.2%}")
    print(f"â­ F1-Score:          {best_res[2]:.4f}")
    
    tp, fp, fn = best_res[3]['matched'], best_res[3]['extra'], best_res[3]['missed']
    print(f"ğŸ“¦ è¯¦æƒ…: æ­£ç¡®æ£€æµ‹(TP)={tp}, è¯¯æ£€(FP)={fp}, æ¼æ£€(FN)={fn}")

if __name__ == "__main__":
    main()