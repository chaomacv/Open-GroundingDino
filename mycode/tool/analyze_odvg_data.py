import json
import os
from collections import defaultdict, Counter
from tqdm import tqdm

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
JSONL_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/train_odvg.jsonl"
# ===============================================

def analyze_data():
    if not os.path.exists(JSONL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {JSONL_PATH}")
        return

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    total_images = 0
    scene_stats = defaultdict(lambda: {"img_count": 0, "labels": Counter()})
    global_label_counts = Counter()

    print(f"ğŸ“– æ­£åœ¨è§£ææ–‡ä»¶: {JSONL_PATH} ...")

    with open(JSONL_PATH, 'r', encoding='utf-8') as f:
        for line in tqdm(f):
            try:
                data = json.loads(line.strip())
                total_images += 1

                # 1. æå–åœºæ™¯ (è·å– filename çš„çˆ¶æ–‡ä»¶å¤¹å)
                filename = data.get("filename", "")
                scene = os.path.dirname(filename)
                if not scene:
                    scene = "Root (æœªåˆ†ç±»)"
                
                scene_stats[scene]["img_count"] += 1

                # 2. æå–ç±»åˆ«ä¿¡æ¯
                # ç»“æ„: detection -> instances -> category
                instances = data.get("detection", {}).get("instances", [])
                for inst in instances:
                    label = inst.get("category", "unknown")
                    # æ›´æ–°åœºæ™¯å†…éƒ¨ç»Ÿè®¡
                    scene_stats[scene]["labels"][label] += 1
                    # æ›´æ–°å…¨å±€ç»Ÿè®¡
                    global_label_counts[label] += 1

            except Exception as e:
                print(f"âš ï¸ è·³è¿‡é”™è¯¯è¡Œ: {e}")

    # ================= ğŸ“Š è¾“å‡ºæŠ¥å‘Š =================
    print("\n" + "="*100)
    print(f"{'ğŸš€ ODVG æ•°æ®é›†åˆ†ææŠ¥å‘Š':^100}")
    print("="*100)
    print(f"ğŸ“ˆ æ€»ä½“è§„æ¨¡:")
    print(f"   - æ€»å›¾ç‰‡æ•°: {total_images}")
    print(f"   - åœºæ™¯æ€»æ•°: {len(scene_stats)}")
    print(f"   - æ¶µç›–ç±»åˆ«æ€»æ•°: {len(global_label_counts)}")
    print("-" * 100)

    # æŒ‰å›¾ç‰‡æ•°é‡æ’åºè¾“å‡ºåœºæ™¯
    print(f"{'ğŸ“‚ æŒ‰åœºæ™¯ç»Ÿè®¡ (Scene Stats)':<40} | {'å›¾ç‰‡æ•°':<8} | {'ç±»åˆ«åˆ†å¸ƒ (Top 3)'}")
    print("-" * 100)
    
    sorted_scenes = sorted(scene_stats.items(), key=lambda x: x[1]['img_count'], reverse=True)
    
    for scene, info in sorted_scenes:
        img_num = info['img_count']
        # è·å–è¯¥åœºæ™¯ä¸‹æ•°é‡æœ€å¤šçš„å‰3ä¸ªç±»åˆ«
        top_labels = info['labels'].most_common(3)
        top_labels_str = ", ".join([f"{k}({v})" for k, v in top_labels])
        
        print(f"{scene:<40} | {img_num:<8} | {top_labels_str}")
        
        # å¦‚æœä½ æƒ³çœ‹è¯¥åœºæ™¯ä¸‹çš„æ‰€æœ‰ç±»åˆ«ï¼Œå–æ¶ˆä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Š
        # for lb, cnt in info['labels'].items():
        #     print(f"      â””â”€ {lb}: {cnt}")

    print("-" * 100)
    print(f"ğŸ·ï¸ å…¨å±€ç±»åˆ«æ±‡æ€» (Global Labels):")
    # æ¯è¡Œæ‰“å° 3 ä¸ªç±»åˆ«ä»¥èŠ‚çœç©ºé—´
    all_labels = global_label_counts.most_common()
    for i in range(0, len(all_labels), 3):
        chunk = all_labels[i:i+3]
        line_str = "  ".join([f"{k:<25}: {v:<6}" for k, v in chunk])
        print(line_str)
    
    print("="*100)

if __name__ == "__main__":
    analyze_data()