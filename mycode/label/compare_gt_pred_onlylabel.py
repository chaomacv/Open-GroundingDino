import json
import os
import numpy as np
import torch
import torchvision.ops.boxes as box_ops
from collections import defaultdict
from tqdm import tqdm

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. çœŸå®æ ‡æ³¨æ–‡ä»¶ (Ground Truth)
GT_JSON_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/benchmark.json"

# 2. ç”Ÿæˆçš„é¢„æµ‹ç»“æœæ–‡ä»¶å¤¹
PRED_DIR = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/mycode/0110_full_test_benchmark"

# 3. è¯„ä¼°å‚æ•°
IOU_THRESHOLD = 0.5

# ===============================================

# 4. æ ¸å¿ƒå…³æ³¨ç™½åå• (åªè¯„ä¼°åˆ—è¡¨ä¸­çš„ 13 ç±»ç¼ºé™·/ç›®æ ‡ï¼Œå¿½ç•¥å…¶ä»–æ‰€æœ‰ç±»åˆ«)
TARGET_LABELS_SET = {
    "fastener_missing",   # æ‰£ä»¶ç¼ºå¤±
    "fastener_crack",     # æ‰£ä»¶æ–­è£‚
    "plate_rust",         # å•å…ƒæ¿é”ˆèš€
    "column_rust",        # ç«‹æŸ±é”ˆèš€
    "mortar_aging",       # ç ‚æµ†å±‚è€åŒ–
    "nut_missing",        # èºæ “ç¼ºå¤±
    "coating_rust",       # æ¶‚å±‚é”ˆèš€
    "coating_peeling",    # æ¶‚å±‚è„±è½
    "guard_rust",         # æ¡¥æ æ†é”ˆèš€
    "nest",               # é¸Ÿå·¢
    "antenna_nut_loose",  # å¤©çº¿èºæ “æ¾åŠ¨
    "plastic_film",       # å¡‘æ–™è†œ
    "rubbish"             # åƒåœ¾
}

# ===============================================

class Evaluator:
    def __init__(self):
        # ä»…ç»Ÿè®¡ç™½åå•å†…çš„ç±»åˆ«
        self.stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0, 'gt_count': 0})

    def update(self, pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels):
        """
        æ ¸å¿ƒæ›´æ–°é€»è¾‘ï¼šå…ˆè¿‡æ»¤ï¼Œå†è¯„ä¼°ã€‚
        """
        
        # --- 1. é¢„å¤„ç†ï¼šå°†æ•°æ®è½¬æ¢ä¸º Tensor ---
        if len(pred_boxes) > 0:
            p_boxes = torch.tensor(pred_boxes, dtype=torch.float32)
            p_scores = torch.tensor(pred_scores, dtype=torch.float32)
            p_labels = np.array(pred_labels)
        else:
            p_boxes = torch.empty((0, 4))
            p_scores = torch.empty((0,))
            p_labels = np.array([])

        if len(gt_boxes) > 0:
            g_boxes = torch.tensor(gt_boxes, dtype=torch.float32)
            g_labels = np.array(gt_labels)
        else:
            g_boxes = torch.empty((0, 4))
            g_labels = np.array([])

        # --- 2. æ ¸å¿ƒè¿‡æ»¤ï¼šåªä¿ç•™ç™½åå•å†…çš„ GT å’Œ Pred ---
        # è¿‡æ»¤ GT
        valid_gt_indices = [i for i, label in enumerate(g_labels) if label in TARGET_LABELS_SET]
        filtered_gt_boxes = g_boxes[valid_gt_indices] if valid_gt_indices else torch.empty((0, 4))
        filtered_gt_labels = g_labels[valid_gt_indices] if valid_gt_indices else np.array([])

        # è¿‡æ»¤ Pred (æ— å…³çš„é¢„æµ‹ç›´æ¥ä¸¢å¼ƒï¼Œä¸ç®— FP)
        valid_pred_indices = [i for i, label in enumerate(p_labels) if label in TARGET_LABELS_SET]
        filtered_pred_boxes = p_boxes[valid_pred_indices] if valid_pred_indices else torch.empty((0, 4))
        filtered_pred_scores = p_scores[valid_pred_indices] if valid_pred_indices else torch.empty((0,))
        filtered_pred_labels = p_labels[valid_pred_indices] if valid_pred_indices else np.array([])

        # --- 3. ç»Ÿè®¡ GT æ•°é‡ ---
        for label in filtered_gt_labels:
            self.stats[label]['gt_count'] += 1

        # --- 4. é€ç±»åˆ«åŒ¹é… ---
        # æ­¤æ—¶å‚ä¸å¾ªç¯çš„åªæœ‰ç™½åå•å†…çš„ç±»åˆ«
        unique_labels = set(filtered_gt_labels) | set(filtered_pred_labels)

        for label in unique_labels:
            # è·å–è¯¥ç±»åˆ«åœ¨è¿‡æ»¤åæ•°æ®ä¸­çš„ç´¢å¼•
            p_idx = [i for i, x in enumerate(filtered_pred_labels) if x == label]
            g_idx = [i for i, x in enumerate(filtered_gt_labels) if x == label]

            curr_p_boxes = filtered_pred_boxes[p_idx] if len(p_idx) > 0 else torch.empty((0, 4))
            curr_p_scores = filtered_pred_scores[p_idx] if len(p_idx) > 0 else torch.empty((0,))
            curr_g_boxes = filtered_gt_boxes[g_idx] if len(g_idx) > 0 else torch.empty((0, 4))

            # Case A: åªæœ‰é¢„æµ‹ï¼Œæ²¡æœ‰GT -> FP
            if len(curr_g_boxes) == 0:
                self.stats[label]['fp'] += len(curr_p_boxes)
                continue
            
            # Case B: åªæœ‰GTï¼Œæ²¡æœ‰é¢„æµ‹ -> FN
            if len(curr_p_boxes) == 0:
                self.stats[label]['fn'] += len(curr_g_boxes)
                continue

            # Case C: è®¡ç®— IoU å¹¶åŒ¹é…
            ious = box_ops.box_iou(curr_p_boxes, curr_g_boxes)
            gt_matched = torch.zeros(len(curr_g_boxes), dtype=torch.bool)
            
            # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½åŒ¹é…
            sorted_indices = torch.argsort(curr_p_scores, descending=True)

            for idx in sorted_indices:
                max_iou, max_gt_idx = torch.max(ious[idx], dim=0)
                if max_iou >= IOU_THRESHOLD and not gt_matched[max_gt_idx]:
                    self.stats[label]['tp'] += 1
                    gt_matched[max_gt_idx] = True
                else:
                    self.stats[label]['fp'] += 1
            
            # å‰©ä½™æœªåŒ¹é…çš„ GT è®¡ä¸º FN
            num_tp = torch.sum(gt_matched).item()
            num_fn = len(curr_g_boxes) - num_tp
            self.stats[label]['fn'] += num_fn

    def print_report(self):
        print("\n" + "="*110)
        print(f"{'ğŸ“Š ä¸“é¡¹ç¼ºé™·è¯„ä¼°æŠ¥å‘Š (Specific Defects Only)':^110}")
        print("="*110)
        print(f"{'Target Class Name':<30} | {'Precision':<10} | {'Recall':<10} | {'GT':<6} | {'TP':<6} | {'FP':<6} | {'FN':<6}")
        print("-" * 110)

        total_tp, total_fp, total_fn, total_gt = 0, 0, 0, 0

        # æŒ‰å­—æ¯é¡ºåºè¾“å‡º
        for label in sorted(list(TARGET_LABELS_SET)):
            s = self.stats[label]
            tp, fp, fn, gt = s['tp'], s['fp'], s['fn'], s['gt_count']
            
            # å³ä½¿å…¨ä¸º0ä¹Ÿè¦æ˜¾ç¤ºï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬å…³æ³¨çš„ç›®æ ‡
            
            total_tp += tp
            total_fp += fp
            total_fn += fn
            total_gt += gt

            precision = tp / (tp + fp + 1e-6)
            recall = tp / (gt + 1e-6)
            
            print(f"{label:<30} | {precision:.4f}     | {recall:.4f}     | {gt:<6} | {tp:<6} | {fp:<6} | {fn:<6}")

        print("-" * 110)
        # è®¡ç®— Micro Average (å…¨å±€ç´¯è®¡)
        all_prec = total_tp / (total_tp + total_fp + 1e-6)
        all_rec = total_tp / (total_gt + 1e-6)
        
        print(f"{'ğŸ† Overall (Target Only)':<30} | {all_prec:.4f}     | {all_rec:.4f}     | {total_gt:<6} | {total_tp:<6} | {total_fp:<6} | {total_fn:<6}")
        print("="*110)

def coco_box_to_xyxy(box):
    x, y, w, h = box
    return [x, y, x + w, y + h]

def main():
    print(f"ğŸ“– æ­£åœ¨åŠ è½½ GT æ–‡ä»¶: {GT_JSON_PATH} ...")
    with open(GT_JSON_PATH, 'r', encoding='utf-8') as f:
        gt_data = json.load(f)
    
    # å»ºç«‹ ID -> Name æ˜ å°„
    cat_id_to_name = {cat['id']: str(cat['name']) for cat in gt_data['categories']}
    
    gt_anns_map = defaultdict(list)
    for ann in gt_data['annotations']:
        gt_anns_map[ann['image_id']].append(ann)

    print(f"âœ… GT åŠ è½½å®Œæˆï¼Œå…± {len(gt_data['images'])} å¼ å›¾ç‰‡ã€‚")
    print(f"ğŸ¯ ä»…è¯„ä¼°ä»¥ä¸‹ {len(TARGET_LABELS_SET)} ç±»ç›®æ ‡: {TARGET_LABELS_SET}")
    
    evaluator = Evaluator()

    print("ğŸš€ å¼€å§‹ä¸“é¡¹è¯„ä¼°...")
    for img_info in tqdm(gt_data['images']):
        file_name = img_info['file_name']
        img_id = img_info['id']
        
        base_name_no_ext = os.path.splitext(os.path.basename(file_name))[0]
        pred_json_name = f"vis_{base_name_no_ext}.json"
        pred_json_path = os.path.join(PRED_DIR, pred_json_name)

        # 1. è§£æ GT
        gt_boxes = []
        gt_labels = []
        for ann in gt_anns_map.get(img_id, []):
            gt_boxes.append(coco_box_to_xyxy(ann['bbox']))
            gt_labels.append(cat_id_to_name.get(ann['category_id'], "unknown"))

        # 2. è§£æ Pred
        pred_boxes = []
        pred_scores = []
        pred_labels = []

        if os.path.exists(pred_json_path):
            try:
                with open(pred_json_path, 'r', encoding='utf-8') as f:
                    pred_data = json.load(f)
                
                for obj in pred_data.get('objects', []):
                    pred_boxes.append(obj['box_pixel_xyxy'])
                    pred_scores.append(obj['score'])
                    # ä¿æŒåŸå§‹æ ‡ç­¾
                    pred_labels.append(str(obj['label']))
            except Exception:
                pass

        evaluator.update(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels)

    evaluator.print_report()

if __name__ == "__main__":
    main()