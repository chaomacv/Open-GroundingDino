import os
import torch
import numpy as np
import cv2
from PIL import Image

# å¼•å…¥ GroundingDINO æ¨¡å—
import groundingdino.datasets.transforms as T
from groundingdino.models import build_model
from groundingdino.util.slconfig import SLConfig
from groundingdino.util.utils import clean_state_dict
from groundingdino.util.inference import predict, annotate

# ================= âš™ï¸ é…ç½®åŒºåŸŸ (éšæ—¶ä¿®æ”¹è¿™é‡Œ) =================

# 1. æƒ³æ‰¾ä»€ä¹ˆï¼Ÿ(Prompt)
# æ ¼å¼è¦æ±‚ï¼šè‹±æ–‡å•è¯ï¼Œç”¨ " . " (ç©ºæ ¼+ç‚¹+ç©ºæ ¼) åˆ†éš”ï¼Œæœ€åä¹Ÿè¦åŠ ç‚¹
# ç¤ºä¾‹ï¼š "insulator . nut_missing . bird_nest ."
TEXT_PROMPT = "rustypaint . corrosion . guard_rust . rustyfence . rustypole . rustyplate ."

# 2. åªæœ‰ä¸€å¼ å›¾ï¼Œè·¯å¾„å¡«è¿™é‡Œ
IMAGE_PATH = "/opt/data/private/xjx/RailMind/é«˜é€Ÿé“è·¯æ— äººæœºå›¾åƒ/FilteredLabeled/å£°å±éšœ-ä»…ç¼ºé™·æ ‡æ³¨-æ£€æµ‹æ¡†/60752222094958_0009_Z_9.JPG"

# 3. ç»“æœä¿å­˜è·¯å¾„
OUTPUT_IMAGE_PATH = "result_single.jpg"

# 4. æ¨¡å‹é…ç½® (ä¿æŒä¸å˜)
CONFIG_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/config/cfg_odvg.py"
CHECKPOINT_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/Open-GroundingDino/logs/0113/model3_only_fullneg/checkpoint_best_regular.pth"
BERT_PATH = "/opt/data/private/xjx/RailMind/agent/RailwayCARS/relatedResearch/GroundingDINO/weights/bert-base-uncased"

# 5. é˜ˆå€¼ (æ ¹æ®æ•ˆæœå¾®è°ƒ)
BOX_THRESHOLD = 0.35   # æ¡†çš„ç½®ä¿¡åº¦é˜ˆå€¼
TEXT_THRESHOLD = 0.35  # æ–‡æœ¬åŒ¹é…é˜ˆå€¼

# =========================================================

def load_model(model_config_path, model_checkpoint_path, device="cuda"):
    args = SLConfig.fromfile(model_config_path)
    
    # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° BERT
    print(f"ğŸ”„ å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° BERT è·¯å¾„: {BERT_PATH}")
    args.text_encoder_type = BERT_PATH
    
    args.device = device
    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    load_res = model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    model.eval()
    return model.to(device)

def load_image(image_path):
    image_pil = Image.open(image_path).convert("RGB")
    transform = T.Compose([
        T.RandomResize([800], max_size=1333),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    image, _ = transform(image_pil, None)
    return image_pil, image

def main():
    # 1. æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
    if not os.path.exists(IMAGE_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡ {IMAGE_PATH}")
        return

    # 2. åŠ è½½æ¨¡å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = load_model(CONFIG_PATH, CHECKPOINT_PATH, device)

    # 3. åŠ è½½å›¾ç‰‡
    print(f"ğŸ–¼ï¸ æ­£åœ¨å¤„ç†å›¾ç‰‡: {IMAGE_PATH}")
    image_source, image = load_image(IMAGE_PATH)

    # 4. æ¨ç†
    print(f"ğŸ” æ£€æµ‹ç›®æ ‡ Prompt: {TEXT_PROMPT}")
    boxes, logits, phrases = predict(
        model=model,
        image=image,
        caption=TEXT_PROMPT,
        box_threshold=BOX_THRESHOLD,
        text_threshold=TEXT_THRESHOLD,
        device=device
    )

    # 5. æ‰“å°æ£€æµ‹ç»“æœåˆ°ç»ˆç«¯
    if len(boxes) > 0:
        print(f"âœ… æ£€æµ‹åˆ° {len(boxes)} ä¸ªç›®æ ‡:")
        for phrase, logit in zip(phrases, logits):
            print(f"   - {phrase}: {logit:.2f}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ã€‚")

    # 6. ç”»å›¾å¹¶ä¿å­˜
    annotated_frame = annotate(image_source=np.asarray(image_source), boxes=boxes, logits=logits, phrases=phrases)
    cv2.imwrite(OUTPUT_IMAGE_PATH, annotated_frame)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_IMAGE_PATH)}")

if __name__ == "__main__":
    main()